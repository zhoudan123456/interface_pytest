"""
æ‹›æ ‡æ–‡ä»¶æ£€æŸ¥å·¥ä½œæµè¯„ä¼°è¿è¡Œè„šæœ¬
ä»æµ‹è¯•å·¥ä½œæµå“åº”ä¸­æå–æ•°æ®å¹¶è¿è¡Œè¯„ä¼°
"""
import json
import os
import sys
import yaml
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from bid_check_evaluation import BidCheckEvaluator


def load_test_workflow_config():
    """åŠ è½½æµ‹è¯•å·¥ä½œæµé…ç½®"""
    config_path = './test_data/bid_check_workflow.yaml'

    if not os.path.exists(config_path):
        print(f"âŒ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
        print("è¯·å…ˆè¿è¡Œæµ‹è¯•å·¥ä½œæµç”Ÿæˆé…ç½®æ–‡ä»¶")
        return None

    with open(config_path, 'r', encoding='utf-8') as f:
        return yaml.safe_load(f)


def find_latest_response(response_type: str) -> dict:
    """
    æŸ¥æ‰¾æœ€æ–°çš„å“åº”æ–‡ä»¶

    Args:
        response_type: 'check_point' æˆ– 'bid_info'

    Returns:
        å“åº”æ•°æ®å­—å…¸
    """
    responses_dir = './test_data/evaluation/responses'

    if not os.path.exists(responses_dir):
        print(f"âŒ å“åº”ç›®å½•ä¸å­˜åœ¨: {responses_dir}")
        print("è¯·å…ˆè¿è¡Œæµ‹è¯•å¹¶ä¿å­˜å“åº”æ•°æ®")
        return None

    # æŸ¥æ‰¾åŒ¹é…çš„æ–‡ä»¶
    files = [
        f for f in os.listdir(responses_dir)
        if f.startswith(f'{response_type}_response_') and f.endswith('.json')
    ]

    if not files:
        print(f"âŒ æœªæ‰¾åˆ° {response_type} å“åº”æ–‡ä»¶")
        test_num = 5 if response_type == "check_point" else 6
        print(f"è¯·è¿è¡Œtest_0{test_num}_* å¹¶ä¿å­˜å“åº”")
        return None

    # æŒ‰æ—¶é—´æ’åºï¼Œå–æœ€æ–°çš„
    files.sort(reverse=True)
    latest_file = os.path.join(responses_dir, files[0])

    print(f"âœ“ æ‰¾åˆ°æœ€æ–°å“åº”æ–‡ä»¶: {files[0]}")

    with open(latest_file, 'r', encoding='utf-8') as f:
        return json.load(f)


def run_evaluation_demo():
    """è¿è¡Œè¯„ä¼°æ¼”ç¤º"""
    print("\n" + "=" * 80)
    print("æ‹›æ ‡æ–‡ä»¶æ£€æŸ¥å·¥ä½œæµè¯„ä¼°".center(80))
    print("=" * 80)

    # åŠ è½½é…ç½®
    config = load_test_workflow_config()
    if not config:
        return

    task_name = config.get('zb_file_name', 'unknown_task')
    print(f"\nğŸ“‹ ä»»åŠ¡åç§°: {task_name}")
    print(f"ğŸ“… è¯„ä¼°æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

    # åˆå§‹åŒ–è¯„ä¼°å™¨
    try:
        evaluator = BidCheckEvaluator()
    except Exception as e:
        print(f"\nâŒ åˆå§‹åŒ–è¯„ä¼°å™¨å¤±è´¥: {e}")
        print("è¯·æ£€æŸ¥ evaluation_config.yaml é…ç½®æ–‡ä»¶")
        return

    # è¯„ä¼°æ£€æŸ¥ç‚¹
    print("\n" + "-" * 80)
    print("è¯„ä¼°1: æ£€æŸ¥ç‚¹å‡†ç¡®æ€§".center(80))
    print("-" * 80)

    check_point_response = find_latest_response('check_point')
    if check_point_response:
        try:
            check_point_result = evaluator.evaluate_check_points(
                check_point_response,
                task_name
            )
            print(f"\nâœ“ æ£€æŸ¥ç‚¹è¯„ä¼°å®Œæˆ")
            print(f"  å‡†ç¡®ç‡: {check_point_result['accuracy_rate']}%")
            print(f"  å¬å›ç‡: {check_point_result['recall_rate']}%")
        except Exception as e:
            print(f"\nâŒ æ£€æŸ¥ç‚¹è¯„ä¼°å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
    else:
        print("\nâš ï¸ è·³è¿‡æ£€æŸ¥ç‚¹è¯„ä¼°")

    # è¯„ä¼°æŠ•æ ‡ä¿¡æ¯
    print("\n" + "-" * 80)
    print("è¯„ä¼°2: æŠ•æ ‡ä¿¡æ¯å‡†ç¡®æ€§".center(80))
    print("-" * 80)

    bid_info_response = find_latest_response('bid_info')
    if bid_info_response:
        try:
            bid_info_result = evaluator.evaluate_bid_info(
                bid_info_response,
                task_name
            )
            print(f"\nâœ“ æŠ•æ ‡ä¿¡æ¯è¯„ä¼°å®Œæˆ")
            print(f"  å‡†ç¡®ç‡: {bid_info_result['accuracy_rate']}%")
        except Exception as e:
            print(f"\nâŒ æŠ•æ ‡ä¿¡æ¯è¯„ä¼°å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
    else:
        print("\nâš ï¸ è·³è¿‡æŠ•æ ‡ä¿¡æ¯è¯„ä¼°")

    # ç”Ÿæˆæ€»ç»“æŠ¥å‘Š
    print("\n" + "=" * 80)
    print("è¯„ä¼°æ€»ç»“".center(80))
    print("=" * 80)

    if check_point_response and bid_info_response:
        print(f"""
âœ“ è¯„ä¼°å®Œæˆï¼

ğŸ“Š è¯„ä¼°ç»“æœ:
  - æ£€æŸ¥ç‚¹å‡†ç¡®ç‡: {check_point_result.get('accuracy_rate', 'N/A')}%
  - æ£€æŸ¥ç‚¹å¬å›ç‡: {check_point_result.get('recall_rate', 'N/A')}%
  - æŠ•æ ‡ä¿¡æ¯å‡†ç¡®ç‡: {bid_info_result.get('accuracy_rate', 'N/A')}%

ğŸ“ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: ./test_data/evaluation/results/
        """)

        # ç”Ÿæˆæ•´ä½“è¯„ä¼°ç­‰çº§
        cp_acc = check_point_result.get('accuracy_rate', 0)
        bi_acc = bid_info_result.get('accuracy_rate', 0)
        avg_acc = (cp_acc + bi_acc) / 2

        if avg_acc >= 80:
            grade = "ä¼˜ç§€ â­â­â­â­â­"
        elif avg_acc >= 70:
            grade = "è‰¯å¥½ â­â­â­â­"
        elif avg_acc >= 60:
            grade = "åŠæ ¼ â­â­â­"
        else:
            grade = "éœ€æ”¹è¿› â­â­"

        print(f"ğŸ¯ æ•´ä½“è¯„çº§: {grade}")

    else:
        print("""
âš ï¸ è¯„ä¼°æœªå®Œæˆ

æç¤º:
  1. è¯·å…ˆè¿è¡Œæµ‹è¯•å·¥ä½œæµ: pytest test_cases/workflows/test_bid_check_workflow.py -v -s
  2. ç¡®ä¿test_05å’Œtest_06å·²æ‰§è¡Œå¹¶æœ‰å“åº”æ•°æ®
  3. å“åº”æ•°æ®ä¼šè‡ªåŠ¨ä¿å­˜åˆ° ./test_data/evaluation/responses/
  4. ç„¶åé‡æ–°è¿è¡Œæ­¤è¯„ä¼°è„šæœ¬
        """)


def save_response_from_test():
    """
    ä»æµ‹è¯•å·¥ä½œæµä¿å­˜å“åº”çš„è¾…åŠ©å‡½æ•°
    åœ¨test_bid_check_workflow.pyä¸­è°ƒç”¨
    """
    def save_response(response_type: str, response_data: dict):
        """ä¿å­˜å“åº”æ•°æ®"""
        import json
        from datetime import datetime

        output_dir = './test_data/evaluation/responses'
        os.makedirs(output_dir, exist_ok=True)

        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        filename = f"{response_type}_response_{timestamp}.json"
        filepath = os.path.join(output_dir, filename)

        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(response_data, f, ensure_ascii=False, indent=2)

        print(f"âœ“ å“åº”å·²ä¿å­˜åˆ°: {filepath}")
        return filepath

    return save_response


if __name__ == '__main__':
    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                              â•‘
â•‘           æ‹›æ ‡æ–‡ä»¶æ£€æŸ¥å·¥ä½œæµè¯„ä¼°æ¡†æ¶ v1.0                                    â•‘
â•‘                                                                              â•‘
â•‘   åŠŸèƒ½: å¯¹æ¯”ç®—æ³•è§£æç»“æœä¸Claudeå¤§æ¨¡å‹å‚è€ƒç­”æ¡ˆï¼Œè¯„ä¼°å‡†ç¡®æ€§                   â•‘
â•‘                                                                              â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)

    print("\nä½¿ç”¨æ–¹æ³•:")
    print("1. è¿è¡Œæµ‹è¯•å·¥ä½œæµå¹¶ä¿å­˜å“åº”:")
    print("   pytest test_cases/workflows/test_bid_check_workflow.py::TestBidCheckWorkflow::test_07_full_workflow_polling -v -s")
    print("\n2. è¿è¡Œæ­¤è„šæœ¬è¿›è¡Œè¯„ä¼°:")
    print("   python run_bid_check_evaluation.py")
    print("\n" + "=" * 80)

    # è¿è¡Œè¯„ä¼°
    run_evaluation_demo()

    print("\n" + "=" * 80)
    print("è¯„ä¼°ç»“æŸ".center(80))
    print("=" * 80 + "\n")
