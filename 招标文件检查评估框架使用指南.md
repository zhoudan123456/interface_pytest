# 招标文件检查工作流评估框架使用指南

## 📋 概述

本评估框架用于对比**算法解析结果**与**Claude大模型参考答案**，评估招标文件检查工作流的准确性。

## 🎯 评估内容

### 1. 检查点评估 (test_05_check_check_point)
- 对比算法提取的检查点与Claude生成的检查点
- 计算准确率、召回率
- 识别完全匹配和部分匹配的检查点

### 2. 投标信息评估 (test_06_get_bid_info)
- 对比算法提取的投标信息与Claude生成的参考信息
- 评估关键字段的匹配度
- 包括：公司名称、投标金额、资质、人员等

## 🚀 使用方法

### 方法1：集成到测试工作流

修改 `test_cases/workflows/test_bid_check_workflow.py`，在测试完成后自动运行评估：

```python
from bid_check_evaluation import BidCheckEvaluator

def test_05_check_check_point(self, api):
    """步骤5: 检查检查点"""
    # ... 原有测试代码 ...

    # 打印响应结果
    print(f"状态码: {res.status_code}")
    response_json = res.json()
    print(f"响应内容: {response_json}")

    # 新增：保存响应用于评估
    self._save_response_for_evaluation('check_point', response_json)

    # 新增：运行评估
    evaluator = BidCheckEvaluator()
    evaluator.evaluate_check_points(response_json, task_name)

def test_06_get_bid_info(self, api):
    """步骤6: 获取投标信息"""
    # ... 原有测试代码 ...

    # 打印响应结果
    print(f"状态码: {res.status_code}")
    response_json = res.json()
    print(f"响应内容: {response_json}")

    # 新增：保存响应用于评估
    self._save_response_for_evaluation('bid_info', response_json)

    # 新增：运行评估
    evaluator = BidCheckEvaluator()
    evaluator.evaluate_bid_info(response_json, task_name)

def _save_response_for_evaluation(self, response_type: str, response_data: dict):
    """保存响应数据到文件"""
    import json
    from datetime import datetime

    output_dir = './test_data/evaluation/responses'
    os.makedirs(output_dir, exist_ok=True)

    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    filename = f"{response_type}_response_{timestamp}.json"
    filepath = os.path.join(output_dir, filename)

    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(response_data, f, ensure_ascii=False, indent=2)

    print(f"✓ 响应已保存到: {filepath}")
```

### 方法2：独立运行评估脚本

```bash
# 运行完整工作流并保存响应
pytest test_cases/workflows/test_bid_check_workflow.py::TestBidCheckWorkflow::test_07_full_workflow_polling -v -s

# 运行评估脚本
python bid_check_evaluation.py
```

### 方法3：使用保存的响应数据

如果已经有保存的API响应，可以直接加载评估：

```python
from bid_check_evaluation import BidCheckEvaluator
import json

# 初始化评估器
evaluator = BidCheckEvaluator()

# 加载检查点响应
with open('./test_data/evaluation/responses/check_point_response_20260129.json', 'r', encoding='utf-8') as f:
    check_point_response = json.load(f)

# 评估检查点
result = evaluator.evaluate_check_points(check_point_response, "招标文件名称")
print(f"检查点准确率: {result['accuracy_rate']}%")

# 加载投标信息响应
with open('./test_data/evaluation/responses/bid_info_response_20260129.json', 'r', encoding='utf-8') as f:
    bid_info_response = json.load(f)

# 评估投标信息
result = evaluator.evaluate_bid_info(bid_info_response, "招标文件名称")
print(f"投标信息准确率: {result['accuracy_rate']}%")
```

## 📁 文件结构

```
interface_pytest/
├── bid_check_evaluation.py           # 评估框架主脚本
├── test_data/
│   └── evaluation/
│       ├── evaluation_config.yaml    # 评估配置（Claude API密钥）
│       ├── responses/                # API响应数据保存目录
│       │   ├── check_point_response_*.json
│       │   └── bid_info_response_*.json
│       └── results/                  # 评估结果保存目录
│           ├── {task_name}_check_points_evaluation_*.json
│           └── {task_name}_bid_info_evaluation_*.json
└── test_cases/
    └── workflows/
        └── test_bid_check_workflow.py  # 测试工作流
```

## 📊 评估结果示例

### 检查点评估结果

```json
{
  "task_name": "孝昌县生物多样性调查.pdf",
  "evaluation_type": "check_points",
  "timestamp": "20260129_153045",
  "evaluation_metrics": {
    "total_algorithm_check_points": 15,
    "total_claude_check_points": 18,
    "matched": 12,
    "partial_matched": 2,
    "accuracy_rate": 66.67,
    "recall_rate": 80.00
  }
}
```

**指标说明：**
- `total_algorithm_check_points`: 算法提取的检查点总数
- `total_claude_check_points`: Claude生成的参考检查点总数
- `matched`: 完全匹配的检查点数
- `partial_matched`: 部分匹配的检查点数
- `accuracy_rate`: 准确率 = matched / Claude检查点数
- `recall_rate`: 召回率 = matched / 算法检查点数

### 投标信息评估结果

```json
{
  "task_name": "孝昌县生物多样性调查.pdf",
  "evaluation_type": "bid_info",
  "timestamp": "20260129_153046",
  "evaluation_metrics": {
    "total_fields": 7,
    "matched_fields": 5,
    "accuracy_rate": 71.43,
    "details": [
      {
        "field": "company_name",
        "algorithm_value": "湖北慧测检测技术有限公司",
        "claude_value": "湖北慧测检测技术有限公司",
        "similarity": 1.0,
        "is_match": true
      },
      {
        "field": "bid_amount",
        "algorithm_value": "150万元",
        "claude_value": "1500000元",
        "similarity": 0.85,
        "is_match": true
      }
    ]
  }
}
```

## ⚙️ 配置说明

### 评估配置文件 (evaluation_config.yaml)

```yaml
# Claude API配置
claude_api_key: "your-claude-api-key"

# 评估参数配置
evaluation:
  # 相似度阈值
  similarity_threshold: 0.8

  # 是否保存详细结果
  save_detailed_results: true

  # 输出目录
  output_dir: ./test_data/evaluation/results
```

## 🔧 高级用法

### 1. 自定义Prompt模板

修改 `bid_check_evaluation.py` 中的 `_get_check_point_prompt()` 和 `_get_bid_info_prompt()` 方法，定制Claude的生成逻辑。

### 2. 自定义相似度算法

修改 `_calculate_similarity()` 方法，使用更复杂的文本相似度算法（如编辑距离、余弦相似度等）。

### 3. 批量评估

```python
from bid_check_evaluation import BidCheckEvaluator
import os
import json

evaluator = BidCheckEvaluator()
responses_dir = './test_data/evaluation/responses'

# 批量评估所有检查点响应
for filename in os.listdir(responses_dir):
    if filename.startswith('check_point_response_'):
        with open(os.path.join(responses_dir, filename), 'r', encoding='utf-8') as f:
            response = json.load(f)

        result = evaluator.evaluate_check_points(response, filename)
        print(f"{filename}: 准确率 {result['accuracy_rate']}%")
```

### 4. 生成评估报告

```python
from bid_check_evaluation import BidCheckEvaluator
import json

evaluator = BidCheckEvaluator()

# 运行评估
check_point_result = evaluator.evaluate_check_points(response, task_name)
bid_info_result = evaluator.evaluate_bid_info(response, task_name)

# 生成Markdown报告
report = f"""
# 招标文件检查评估报告

## 任务信息
- 任务名称: {task_name}
- 评估时间: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## 检查点评估
- 算法检查点数: {check_point_result['total_algorithm_check_points']}
- 参考检查点数: {check_point_result['total_claude_check_points']}
- 准确率: {check_point_result['accuracy_rate']}%
- 召回率: {check_point_result['recall_rate']}%

## 投标信息评估
- 总字段数: {bid_info_result['total_fields']}
- 匹配字段数: {bid_info_result['matched_fields']}
- 准确率: {bid_info_result['accuracy_rate']}%

## 总结
整体评估: {'通过' if check_point_result['accuracy_rate'] > 70 and bid_info_result['accuracy_rate'] > 70 else '需改进'}
"""

with open(f'./test_data/evaluation/reports/{task_name}_report.md', 'w', encoding='utf-8') as f:
    f.write(report)

print("✓ 评估报告已生成")
```

## 📝 注意事项

1. **Claude API费用**: 评估会调用Claude API，产生费用，建议先小规模测试
2. **响应数据格式**: 需要根据实际API响应调整提取逻辑
3. **Prompt优化**: Claude的生成结果依赖于Prompt质量，可能需要多次调试
4. **文件路径**: 确保所有路径配置正确，文件可访问

## 🎓 最佳实践

1. **定期评估**: 每次算法更新后运行评估，跟踪改进情况
2. **对比历史**: 保存历史评估结果，对比不同版本的算法性能
3. **错误分析**: 分析未匹配的检查点/字段，找出算法改进方向
4. **阈值调优**: 根据业务需求调整相似度阈值

## 📚 相关文档

- [招标文件解析准确度测试框架](./招标文件解析准确度测试框架.md)
- [HAR文件处理使用指南](./HAR文件处理使用指南.md)
- [API客户端使用说明](./api_clients/README.md)
