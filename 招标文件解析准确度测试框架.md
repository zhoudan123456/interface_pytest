# æ‹›æ ‡æ–‡ä»¶è§£æå‡†ç¡®åº¦æµ‹è¯•æ¡†æ¶ - é¡¹ç›®ç»“æ„è¯´æ˜

## ğŸ“ é¡¹ç›®ç»“æ„

```
interface_pytest/
â”‚
â”œâ”€â”€ api_clients/                          # APIå®¢æˆ·ç«¯æ¨¡å—
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ claude_client.py                 # Claude APIå®¢æˆ·ç«¯(ç”Ÿæˆå‚è€ƒç­”æ¡ˆå’Œè¯„ä¼°)
â”‚   â””â”€â”€ algorithm_client.py              # ç®—æ³•æ¨¡å‹APIå®¢æˆ·ç«¯(è°ƒç”¨è§£æAPI)
â”‚
â”œâ”€â”€ evaluators/                           # è¯„ä¼°å™¨æ¨¡å—
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ claude_evaluator.py              # Claudeè¯„ä¼°å™¨(è¯„ä¼°ç®—æ³•è¾“å‡ºè´¨é‡)
â”‚
â”œâ”€â”€ processors/                           # æ•°æ®å¤„ç†å™¨æ¨¡å—
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ document_processor.py            # æ–‡æ¡£å¤„ç†å™¨(åŠ è½½å’Œé¢„å¤„ç†æ–‡æ¡£)
â”‚
â”œâ”€â”€ test_cases/                           # æµ‹è¯•ç”¨ä¾‹
â”‚   â””â”€â”€ test_bid_parser_evaluation.py    # è¯„ä¼°æµ‹è¯•ç”¨ä¾‹
â”‚
â”œâ”€â”€ test_data/evaluation/                 # æµ‹è¯•æ•°æ®ç›®å½•
â”‚   â”œâ”€â”€ input/                           # è¾“å…¥æ–‡æ¡£ç›®å½•(æ”¾ç½®å¾…æµ‹è¯•çš„æ‹›æ ‡æ–‡ä»¶)
â”‚   â”œâ”€â”€ output/                          # è¾“å‡ºç»“æœç›®å½•(è¯„ä¼°ç»“æœå’ŒæŠ¥å‘Š)
â”‚   â”œâ”€â”€ evaluation_config.yaml           # è¯„ä¼°é…ç½®æ–‡ä»¶
â”‚   â””â”€â”€ README.md                        # ä½¿ç”¨è¯´æ˜æ–‡æ¡£
â”‚
â”œâ”€â”€ config/prompts/                       # Claudeæç¤ºè¯æ¨¡æ¿
â”‚   â”œâ”€â”€ reference_generation.txt         # å‚è€ƒç­”æ¡ˆç”Ÿæˆæç¤ºè¯
â”‚   â””â”€â”€ evaluation.txt                   # è¯„ä¼°æç¤ºè¯
â”‚
â”œâ”€â”€ bid_evaluation_pipeline.py           # ä¸»æµç¨‹æ§åˆ¶å™¨
â”œâ”€â”€ run_evaluation.py                    # å¿«é€Ÿå¼€å§‹è„šæœ¬
â”œâ”€â”€ requirements_evaluation.txt          # ä¾èµ–åŒ…åˆ—è¡¨
â””â”€â”€ æ‹›æ ‡æ–‡ä»¶è§£æå‡†ç¡®åº¦æµ‹è¯•æ¡†æ¶.md         # æœ¬æ–‡æ¡£
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. å®‰è£…ä¾èµ–

```bash
pip install -r requirements_evaluation.txt
```

### 2. é…ç½®Claude APIå¯†é’¥

**æ–¹å¼1: ç¯å¢ƒå˜é‡(æ¨è)**
```bash
# Windows
set CLAUDE_API_KEY=your-api-key-here

# Linux/Mac
export CLAUDE_API_KEY=your-api-key-here
```

**æ–¹å¼2: ä¿®æ”¹é…ç½®æ–‡ä»¶**
ç¼–è¾‘ `test_data/evaluation/evaluation_config.yaml`:
```yaml
claude_api_key: "your-api-key-here"
```

### 3. å‡†å¤‡æµ‹è¯•æ–‡æ¡£

å°†å¾…æµ‹è¯•çš„æ‹›æ ‡æ–‡ä»¶æ”¾å…¥ `test_data/evaluation/input/` ç›®å½•:
```bash
cp your_bid_file.txt test_data/evaluation/input/
```

### 4. è¿è¡Œæµ‹è¯•

**æ–¹å¼1: ä½¿ç”¨å¿«é€Ÿå¼€å§‹è„šæœ¬**
```bash
python run_evaluation.py
```

**æ–¹å¼2: ä½¿ç”¨pytest**
```bash
# è¿è¡Œæ‰€æœ‰è¯„ä¼°æµ‹è¯•
pytest test_cases/test_bid_parser_evaluation.py -v

# è¿è¡Œç‰¹å®šæµ‹è¯•
pytest test_cases/test_bid_parser_evaluation.py::TestBidParserEvaluationIntegration::test_full_evaluation_workflow -v
```

## ğŸ“Š æ ¸å¿ƒåŠŸèƒ½è¯´æ˜

### 1. APIå®¢æˆ·ç«¯æ¨¡å—

#### Claudeå®¢æˆ·ç«¯ (`api_clients/claude_client.py`)
- `generate_reference_checkpoints()`: ç”Ÿæˆæ ‡å‡†å‚è€ƒç­”æ¡ˆ
- `evaluate_checkpoints()`: è¯„ä¼°ç®—æ³•è¾“å‡ºè´¨é‡

#### ç®—æ³•å®¢æˆ·ç«¯ (`api_clients/algorithm_client.py`)
- `parse_bid_document()`: è°ƒç”¨ç®—æ³•APIè§£ææ‹›æ ‡æ–‡ä»¶

### 2. è¯„ä¼°å™¨æ¨¡å—

#### Claudeè¯„ä¼°å™¨ (`evaluators/claude_evaluator.py`)
- `evaluate()`: è¯„ä¼°å•ä¸ªæ–‡æ¡£
- `evaluate_batch()`: æ‰¹é‡è¯„ä¼°
- `generate_evaluation_report()`: ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š

**è¯„ä¼°æŒ‡æ ‡:**
- å®Œæ•´æ€§è¯„åˆ†(Completeness): æ˜¯å¦é—æ¼å…³é”®ä¿¡æ¯
- å‡†ç¡®æ€§è¯„åˆ†(Accuracy): æå–ä¿¡æ¯æ˜¯å¦æ­£ç¡®
- ä¸€è‡´æ€§è¯„åˆ†(Consistency): ä¸å‚è€ƒç­”æ¡ˆçš„åŒ¹é…åº¦
- ç»Ÿè®¡æŒ‡æ ‡: Precision, Recall, F1-Score

### 3. æ–‡æ¡£å¤„ç†å™¨ (`processors/document_processor.py`)

**æ”¯æŒçš„æ–‡ä»¶æ ¼å¼:**
- TXT: æ–‡æœ¬æ–‡ä»¶
- PDF: PDFæ–‡æ¡£(éœ€è¦PyPDF2)
- DOCX: Wordæ–‡æ¡£(éœ€è¦python-docx)

**ä¸»è¦åŠŸèƒ½:**
- `load_and_preprocess()`: åŠ è½½å¹¶é¢„å¤„ç†æ–‡æ¡£
- `load_batch()`: æ‰¹é‡åŠ è½½æ–‡æ¡£
- `extract_text_sections()`: æå–ç‰¹å®šç« èŠ‚

### 4. ä¸»æµç¨‹æ§åˆ¶å™¨ (`bid_evaluation_pipeline.py`)

**æ ¸å¿ƒæ–¹æ³•:**
```python
pipeline = BidParserEvaluationPipeline(config)

# è¯„ä¼°å•ä¸ªæ–‡æ¡£
result = pipeline.evaluate_single_document(document_path, document_id)

# æ‰¹é‡è¯„ä¼°
results = pipeline.evaluate_batch(documents_list)

# è¯„ä¼°æ•´ä¸ªç›®å½•
results = pipeline.evaluate_directory(directory, pattern)
```

## ğŸ“ ä½¿ç”¨ç¤ºä¾‹

### ç¤ºä¾‹1: å¿«é€Ÿè¯„ä¼°å•ä¸ªæ–‡æ¡£

```python
from bid_evaluation_pipeline import BidParserEvaluationPipeline

# åˆ›å»ºæµæ°´çº¿
pipeline = BidParserEvaluationPipeline()

# è¯„ä¼°æ–‡æ¡£
result = pipeline.evaluate_single_document(
    document_path='./test_data/evaluation/input/sample.txt',
    document_id='doc-123'  # ä»ä¸Šä¼ æ¥å£è·å–çš„ID
)

# æŸ¥çœ‹ç»“æœ
print(f"æ€»ä½“è¯„åˆ†: {result['overall_score']}")
print(f"å®Œæ•´æ€§: {result['completeness_score']}")
print(f"å‡†ç¡®æ€§: {result['accuracy_score']}")
print(f"F1åˆ†æ•°: {result['f1_score']}")
```

### ç¤ºä¾‹2: æ‰¹é‡è¯„ä¼°

```python
from bid_evaluation_pipeline import BidParserEvaluationPipeline

pipeline = BidParserEvaluationPipeline()

# å‡†å¤‡æ–‡æ¡£åˆ—è¡¨
documents = [
    {'path': './input/doc1.txt', 'document_id': 'id1'},
    {'path': './input/doc2.txt', 'document_id': 'id2'},
]

# æ‰¹é‡è¯„ä¼°
results = pipeline.evaluate_batch(documents)

# ç”ŸæˆæŠ¥å‘Š
report = pipeline.evaluator.generate_evaluation_report(results)
print(report)
```

### ç¤ºä¾‹3: åœ¨pytestä¸­ä½¿ç”¨

```python
# test_cases/test_my_evaluation.py
import pytest
from bid_evaluation_pipeline import BidParserEvaluationPipeline

def test_evaluation():
    pipeline = BidParserEvaluationPipeline()

    result = pipeline.evaluate_single_document(
        document_path='./test_data/evaluation/input/sample.txt',
        document_id='doc-123'
    )

    # æ–­è¨€
    assert result['overall_score'] >= 60
    assert result['f1_score'] >= 70
```

## ğŸ“„ è¾“å‡ºç»“æœè¯´æ˜

### 1. å•ä¸ªæ–‡æ¡£ç»“æœ (`{filename}_result.json`)

```json
{
  "algorithm_output": [...],           // ç®—æ³•æ¨¡å‹çš„è¾“å‡º
  "reference_checkpoints": [...],      // Claudeç”Ÿæˆçš„å‚è€ƒç­”æ¡ˆ
  "evaluation_result": {
    "overall_score": 85,               // æ€»ä½“è¯„åˆ†
    "completeness_score": 80,          // å®Œæ•´æ€§è¯„åˆ†
    "accuracy_score": 90,              // å‡†ç¡®æ€§è¯„åˆ†
    "consistency_score": 85,           // ä¸€è‡´æ€§è¯„åˆ†
    "precision": 88.5,                 // ç²¾ç¡®ç‡
    "recall": 82.3,                    // å¬å›ç‡
    "f1_score": 85.3,                  // F1åˆ†æ•°
    "missing_checkpoints": [...],      // é—æ¼çš„æ£€æŸ¥ç‚¹
    "incorrect_checkpoints": [...],    // é”™è¯¯çš„æ£€æŸ¥ç‚¹
    "suggestions": [...]               // æ”¹è¿›å»ºè®®
  }
}
```

### 2. æ‰¹é‡è¯„ä¼°æŠ¥å‘Š (`evaluation_report.txt`)

```
=== æ‹›æ ‡æ–‡ä»¶è§£æå‡†ç¡®åº¦è¯„ä¼°æŠ¥å‘Š ===

è¯„ä¼°æ–‡æ¡£æ•°é‡: 5

=== å¹³å‡åˆ†æ•° ===
æ€»ä½“è¯„åˆ†: 82.50
å®Œæ•´æ€§: 80.00
å‡†ç¡®æ€§: 85.00
F1åˆ†æ•°: 82.50

=== è¯¦ç»†ç»“æœ ===
æ–‡æ¡£ 1:
  æ€»ä½“è¯„åˆ†: 85
  å®Œæ•´æ€§: 80
  å‡†ç¡®æ€§: 90
  F1åˆ†æ•°: 85
...
```

## âš™ï¸ é…ç½®è¯´æ˜

é…ç½®æ–‡ä»¶: `test_data/evaluation/evaluation_config.yaml`

```yaml
# Claude APIé…ç½®
claude_api_key: ${CLAUDE_API_KEY}    # ä»ç¯å¢ƒå˜é‡è¯»å–

# ç®—æ³•APIç¯å¢ƒ
algorithm_env: Test_Env               # æˆ– Prod_Env

# è¾“å‡ºç›®å½•
output_dir: ./test_data/evaluation/output

# è¯„ä¼°é˜ˆå€¼
evaluation:
  min_overall_score: 60               # æœ€ä½åˆæ ¼åˆ†æ•°
  min_completeness_score: 60
  min_accuracy_score: 60
```

## ğŸ”§ è‡ªå®šä¹‰æ‰©å±•

### è‡ªå®šä¹‰è¯„ä¼°å™¨

```python
from evaluators.claude_evaluator import ClaudeEvaluator

class MyCustomEvaluator(ClaudeEvaluator):
    def evaluate(self, document_text, algorithm_checkpoints, reference_checkpoints):
        result = super().evaluate(document_text, algorithm_checkpoints, reference_checkpoints)

        # æ·»åŠ è‡ªå®šä¹‰è¯„ä¼°æŒ‡æ ‡
        result['my_custom_score'] = self._calculate_custom_score(...)

        return result
```

### è‡ªå®šä¹‰æç¤ºè¯

ç¼–è¾‘ `config/prompts/` ä¸‹çš„æç¤ºè¯æ¨¡æ¿ä»¥é€‚åº”ç‰¹å®šéœ€æ±‚ã€‚

## âš ï¸ æ³¨æ„äº‹é¡¹

1. **APIå¯†é’¥**: å¿…é¡»é…ç½®æœ‰æ•ˆçš„Claude APIå¯†é’¥
2. **æ–‡æ¡£ID**: ç®—æ³•APIéœ€è¦å…ˆä¸Šä¼ æ–‡æ¡£è·å–ID
3. **å¹¶å‘é™åˆ¶**: Claude APIæœ‰é€Ÿç‡é™åˆ¶
4. **æˆæœ¬æ§åˆ¶**: APIæŒ‰tokenè®¡è´¹,å¤§é‡æµ‹è¯•ä¼šäº§ç”Ÿè´¹ç”¨
5. **æ–‡ä»¶ç¼–ç **: ç¡®ä¿æ–‡æ¡£ç¼–ç ä¸ºUTF-8

## ğŸ› æ•…éšœæ’é™¤

### é—®é¢˜1: APIå¯†é’¥æœªè®¾ç½®
```
âŒ é”™è¯¯: æœªè®¾ç½®CLAUDE_API_KEYç¯å¢ƒå˜é‡
```
**è§£å†³æ–¹æ¡ˆ**: è®¾ç½®ç¯å¢ƒå˜é‡æˆ–ä¿®æ”¹é…ç½®æ–‡ä»¶

### é—®é¢˜2: æ–‡æ¡£IDä¸å­˜åœ¨
```
âŒ é”™è¯¯: éœ€è¦æ–‡æ¡£ID,è¯·å…ˆè¿è¡Œä¸Šä¼ æ–‡æ¡£æµ‹è¯•
```
**è§£å†³æ–¹æ¡ˆ**: å…ˆè¿è¡Œä¸Šä¼ æµ‹è¯•è·å–æ–‡æ¡£ID
```bash
pytest test_cases/workflows/test_bid_workflow.py::TestBidGenerateWorkflow::test_01_upload_document -v
```

### é—®é¢˜3: å¯¼å…¥æ¨¡å—å¤±è´¥
```
ModuleNotFoundError: No module named 'xxx'
```
**è§£å†³æ–¹æ¡ˆ**: å®‰è£…ç¼ºå¤±çš„ä¾èµ–
```bash
pip install -r requirements_evaluation.txt
```

## ğŸ“š ç›¸å…³æ–‡æ¡£

- [æ–¹æ¡ˆè®¾è®¡æ–‡æ¡£](./è§£æå‡†ç¡®åº¦æµ‹è¯•.md)
- [ä½¿ç”¨è¯´æ˜](./test_data/evaluation/README.md)
- [APIå®¢æˆ·ç«¯å®ç°](./api_clients/)
- [è¯„ä¼°å™¨å®ç°](./evaluators/)

## ğŸ“ æŠ€æœ¯æ”¯æŒ

å¦‚æœ‰é—®é¢˜,è¯·æŸ¥çœ‹:
1. æœ¬æ–‡æ¡£
2. `test_data/evaluation/README.md`
3. æµ‹è¯•ç”¨ä¾‹: `test_cases/test_bid_parser_evaluation.py`

---

**ç¥æ‚¨ä½¿ç”¨æ„‰å¿«!** ğŸ‰
